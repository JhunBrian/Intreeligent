{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1fc2f43-ec53-431c-ae4d-f26f41c3e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from tqdm.notebook import tnrange, tqdm_notebook\n",
    "\n",
    "# standard libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import json\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from dataset import TreeDataset\n",
    "from transforms import MaskResize\n",
    "from visualize import BatchVisualizer as BV\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb329d2a-5484-4485-9e3b-510fcc4cb4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_configs = {\n",
    "    'root': '../data/Some Trees Dataset.v12i.coco-segmentation',\n",
    "    'seed': 42,\n",
    "    'transforms': {\n",
    "        'image_transforms': T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Resize(size=(512, 512))\n",
    "        ]),\n",
    "        'mask_transforms': MaskResize(size=(512, 512))\n",
    "    },\n",
    "    'batch_size': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33759403-888b-4440-b750-e7140ddceac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.set_seed(data_configs['seed'])\n",
    "train_set = TreeDataset(data_configs['root'], \n",
    "                        'train', \n",
    "                        data_configs['transforms']['image_transforms'], \n",
    "                        data_configs['transforms']['mask_transforms']\n",
    ")\n",
    "\n",
    "val_set = TreeDataset(data_configs['root'], \n",
    "                        'valid', \n",
    "                        data_configs['transforms']['image_transforms'], \n",
    "                        data_configs['transforms']['mask_transforms']\n",
    ")\n",
    "\n",
    "test_set = TreeDataset(data_configs['root'], \n",
    "                        'test', \n",
    "                        data_configs['transforms']['image_transforms'], \n",
    "                        data_configs['transforms']['mask_transforms']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b770f7c9-890c-4dc6-9d02-ef26aeaecabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.set_seed(data_configs['seed'])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, targets = zip(*batch)\n",
    "    return list(images), list(targets)\n",
    "    \n",
    "train_loader = DataLoader(train_set, batch_size=data_configs['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_set, batch_size=data_configs['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=data_configs['batch_size'], shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e916087a-2dbb-4f0f-ad3d-31e45bff5dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instance_seg_lightning.py\n",
    "import os\n",
    "from typing import Callable, List, Dict, Any, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# ---------- PredictionProcessor (your provided implementation, extended slightly) ----------\n",
    "class PredictionProcessor:\n",
    "    def __init__(self, mask_threshold: float = 0.5, score_threshold: float = 0.5):\n",
    "        self.mask_threshold = float(mask_threshold)\n",
    "        self.score_threshold = float(score_threshold)\n",
    "\n",
    "    def __call__(self, prediction: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        # Handle empty predictions\n",
    "        if not prediction or \"masks\" not in prediction or \"scores\" not in prediction:\n",
    "            return {\n",
    "                \"boxes\": torch.empty((0, 4)),\n",
    "                \"labels\": torch.empty((0,), dtype=torch.int64),\n",
    "                \"scores\": torch.empty((0,)),\n",
    "                \"masks\": torch.empty((0, 1, 1, 1))\n",
    "            }\n",
    "\n",
    "        keep = prediction[\"scores\"] >= self.score_threshold\n",
    "        if keep.numel() == 0:\n",
    "            # nothing kept\n",
    "            return {\n",
    "                \"boxes\": torch.empty((0, 4)),\n",
    "                \"labels\": torch.empty((0,), dtype=torch.int64),\n",
    "                \"scores\": torch.empty((0,)),\n",
    "                \"masks\": torch.empty((0, 1, 1, 1))\n",
    "            }\n",
    "\n",
    "        prediction = {k: v[keep] for k, v in prediction.items()}\n",
    "\n",
    "        if \"masks\" in prediction and prediction[\"masks\"].numel() > 0:\n",
    "            # Ensure masks are binary float [N,1,H,W]\n",
    "            masks = prediction[\"masks\"]\n",
    "            # Some models return masks as [N, H, W]\n",
    "            if masks.dim() == 3:\n",
    "                masks = masks.unsqueeze(1)\n",
    "            prediction[\"masks\"] = (masks > self.mask_threshold).float()\n",
    "\n",
    "        return prediction\n",
    "\n",
    "\n",
    "# ---------- Utilities for mask IoU and matching ----------\n",
    "def mask_iou_matrix(pred_masks: torch.Tensor, gt_masks: torch.Tensor, eps: float = 1e-7) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    pred_masks: [P, H, W] or [P,1,H,W]\n",
    "    gt_masks: [G, H, W] or [G,1,H,W]\n",
    "    returns IoU matrix [P, G]\n",
    "    \"\"\"\n",
    "    if pred_masks.dim() == 4:\n",
    "        pred_masks = pred_masks[:, 0]\n",
    "    if gt_masks.dim() == 4:\n",
    "        gt_masks = gt_masks[:, 0]\n",
    "\n",
    "    if pred_masks.numel() == 0 or gt_masks.numel() == 0:\n",
    "        return torch.zeros((pred_masks.shape[0], gt_masks.shape[0]), dtype=torch.float32, device=pred_masks.device)\n",
    "\n",
    "    P = pred_masks.shape[0]\n",
    "    G = gt_masks.shape[0]\n",
    "\n",
    "    pred_flat = pred_masks.reshape(P, -1).float()\n",
    "    gt_flat = gt_masks.reshape(G, -1).float()\n",
    "\n",
    "    # intersections: P x G\n",
    "    inter = (pred_flat[:, None, :] * gt_flat[None, :, :]).sum(dim=2)  # (P, G)\n",
    "    union = (pred_flat.sum(dim=1)[:, None] + gt_flat.sum(dim=1)[None, :] - inter).clamp(min=eps)\n",
    "    iou = inter / union\n",
    "    return iou\n",
    "\n",
    "\n",
    "def compute_pairwise_iou(pred: Dict[str, torch.Tensor], gt: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convenience: get IoU matrix between prediction masks and GT masks for a single image.\n",
    "    \"\"\"\n",
    "    pred_masks = pred.get(\"masks\", torch.empty((0, 1, 1, 1)))\n",
    "    gt_masks = gt.get(\"masks\", torch.empty((0, 1, 1, 1)))\n",
    "\n",
    "    # ensure shapes [N,1,H,W] and [M,1,H,W] or empty\n",
    "    if pred_masks.dim() == 3:\n",
    "        pred_masks = pred_masks.unsqueeze(1)\n",
    "    if gt_masks.dim() == 3:\n",
    "        gt_masks = gt_masks.unsqueeze(1)\n",
    "\n",
    "    # convert boolean to float\n",
    "    return mask_iou_matrix(pred_masks, gt_masks)\n",
    "\n",
    "\n",
    "# ---------- Metrics container ----------\n",
    "class InstanceSegMetrics:\n",
    "    \"\"\"\n",
    "    Computes metrics over multiple images. Use update(prediction, target) per image.\n",
    "    At the end call compute() to get aggregated metrics.\n",
    "    \"\"\"\n",
    "    def __init__(self, iou_thresholds: Optional[List[float]] = None):\n",
    "        # IoU thresholds for mAP approximation\n",
    "        if iou_thresholds is None:\n",
    "            # COCO-like thresholds 0.5 : 0.05 : 0.95\n",
    "            self.iou_thresholds = [round(x, 2) for x in np.arange(0.5, 0.96, 0.05)]\n",
    "        else:\n",
    "            self.iou_thresholds = iou_thresholds\n",
    "\n",
    "        # accumulators\n",
    "        self.total_gt = 0\n",
    "        self.total_pred = 0\n",
    "        self.matched_gts_by_thresh = {t: 0 for t in self.iou_thresholds}  # count of GTs matched at each threshold\n",
    "        self.total_matches = 0  # matched pairs at IoU 0.5 used for IoU average etc.\n",
    "        self.sum_iou = 0.0\n",
    "        self.count_iou = 0\n",
    "\n",
    "        # for precision/recall counting (at IoU=0.5 default)\n",
    "        self.tp = 0\n",
    "        self.fp = 0\n",
    "        self.fn = 0\n",
    "\n",
    "    def update(self, prediction: Dict[str, torch.Tensor], gt: Dict[str, torch.Tensor], match_iou_for_relaxed: float = 0.5):\n",
    "        \"\"\"\n",
    "        Update accumulators for one image.\n",
    "        prediction: processed prediction dict (masks binary, boxes, scores, labels)\n",
    "        gt: ground truth dict (masks)\n",
    "        \"\"\"\n",
    "        # Get masks\n",
    "        pred_masks = prediction.get(\"masks\", torch.empty((0, 1, 1, 1)))\n",
    "        gt_masks = gt.get(\"masks\", torch.empty((0, 1, 1, 1)))\n",
    "\n",
    "        # normalize dims\n",
    "        if pred_masks.dim() == 3:\n",
    "            pred_masks = pred_masks.unsqueeze(1)\n",
    "        if gt_masks.dim() == 3:\n",
    "            gt_masks = gt_masks.unsqueeze(1)\n",
    "\n",
    "        P = pred_masks.shape[0]\n",
    "        G = gt_masks.shape[0]\n",
    "\n",
    "        self.total_gt += G\n",
    "        self.total_pred += P\n",
    "\n",
    "        if P == 0 and G == 0:\n",
    "            return\n",
    "\n",
    "        # IoU matrix shape [P, G]\n",
    "        iou_mat = mask_iou_matrix(pred_masks, gt_masks)\n",
    "\n",
    "        # For each IoU threshold, count matched GTs (relaxed: a GT is matched if any pred IoU >= thresh)\n",
    "        for t in self.iou_thresholds:\n",
    "            matched_gts = (iou_mat >= t).any(dim=0).sum().item()\n",
    "            self.matched_gts_by_thresh[t] += int(matched_gts)\n",
    "\n",
    "        # For standard precision/recall at IoU=0.5: greedily match preds -> gts to count TP/FP/FN\n",
    "        thresh = 0.5\n",
    "        if iou_mat.numel() > 0:\n",
    "            # greedy matching: find best matches\n",
    "            iou_copy = iou_mat.clone()\n",
    "            matched_pred = torch.zeros(P, dtype=torch.bool, device=iou_mat.device)\n",
    "            matched_gt = torch.zeros(G, dtype=torch.bool, device=iou_mat.device)\n",
    "\n",
    "            while True:\n",
    "                val, idxs = iou_copy.max(dim=1)  # best GT for each pred\n",
    "                pred_best_iou, pred_best_gt = torch.max(val, dim=0)\n",
    "                # Actually simpler: find global max\n",
    "                if iou_copy.numel() == 0:\n",
    "                    break\n",
    "                global_val = iou_copy.max()\n",
    "                if global_val < thresh:\n",
    "                    break\n",
    "                # get indices\n",
    "                max_idx = (iou_copy == global_val).nonzero()[0]\n",
    "                p_idx = max_idx[0].item()\n",
    "                g_idx = max_idx[1].item()\n",
    "                # mark matched\n",
    "                matched_pred[p_idx] = True\n",
    "                matched_gt[g_idx] = True\n",
    "                # zero out row and column\n",
    "                iou_copy[p_idx, :] = -1\n",
    "                iou_copy[:, g_idx] = -1\n",
    "\n",
    "            tp = int(matched_gt.sum().item())\n",
    "            fp = int((~matched_pred).sum().item())\n",
    "            fn = int((~matched_gt).sum().item())\n",
    "\n",
    "            self.tp += tp\n",
    "            self.fp += fp\n",
    "            self.fn += fn\n",
    "\n",
    "            # gather IoU stats for matched pairs for reporting average IoU\n",
    "            # compute IoU for matched preds->gt (we'll take max per GT)\n",
    "            if G > 0:\n",
    "                best_per_gt, _ = iou_mat.max(dim=0)  # best pred IoU for each gt\n",
    "                matched_mask = best_per_gt >= 0  # any pred\n",
    "                # sum IoU of matched ones\n",
    "                matched_ious = best_per_gt[matched_mask]\n",
    "                self.sum_iou += matched_ious.sum().item()\n",
    "                self.count_iou += matched_ious.numel()\n",
    "\n",
    "    def compute(self) -> Dict[str, float]:\n",
    "        # relaxed recall per threshold\n",
    "        relaxed_recall = {}\n",
    "        for t in self.iou_thresholds:\n",
    "            total_gt = max(self.total_gt, 1)\n",
    "            relaxed_recall[f\"relaxed_recall_iou_{t:.2f}\"] = float(self.matched_gts_by_thresh[t] / total_gt)\n",
    "\n",
    "        # average IoU across matched instances (if any)\n",
    "        mean_iou = float(self.sum_iou / self.count_iou) if self.count_iou > 0 else 0.0\n",
    "\n",
    "        # precision/recall/f1 at IoU=0.5 using counted TP/FP/FN\n",
    "        precision = float(self.tp / (self.tp + self.fp)) if (self.tp + self.fp) > 0 else 0.0\n",
    "        recall = float(self.tp / (self.tp + self.fn)) if (self.tp + self.fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "        # simplified mAP: mean over thresholds of matched_gt / total_gt (rough approximation)\n",
    "        map_simple = float(np.mean([self.matched_gts_by_thresh[t] / max(self.total_gt, 1) for t in self.iou_thresholds]))\n",
    "\n",
    "        return {\n",
    "            \"mean_iou_matched\": mean_iou,\n",
    "            \"precision_iou_0.5\": precision,\n",
    "            \"recall_iou_0.5\": recall,\n",
    "            \"f1_iou_0.5\": f1,\n",
    "            \"map_simple\": map_simple,\n",
    "            **relaxed_recall\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "        self.total_gt = 0\n",
    "        self.total_pred = 0\n",
    "        self.matched_gts_by_thresh = {t: 0 for t in self.iou_thresholds}\n",
    "        self.total_matches = 0\n",
    "        self.sum_iou = 0.0\n",
    "        self.count_iou = 0\n",
    "        self.tp = 0\n",
    "        self.fp = 0\n",
    "        self.fn = 0\n",
    "\n",
    "\n",
    "# ---------- LightningModule ----------\n",
    "class InstanceSegLightningModule(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        lr: float = 1e-4,\n",
    "        weight_decay: float = 1e-4,\n",
    "        prediction_processor: Optional[PredictionProcessor] = None,\n",
    "        inv_transform: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\n",
    "        mask_threshold: float = 0.5,\n",
    "        score_threshold: float = 0.5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        model: your Mask R-CNN model (already constructed outside)\n",
    "        inv_transform: function to inverse any preprocess (accepts tensor image and returns numpy image HxWxC or PIL)\n",
    "        prediction_processor: instance of PredictionProcessor (if None, one is created)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.pred_proc = prediction_processor or PredictionProcessor(mask_threshold=mask_threshold, score_threshold=score_threshold)\n",
    "        self.inv_transform = inv_transform or (lambda x: x)  # identity if not provided\n",
    "\n",
    "        # For accumulating validation images/preds\n",
    "        self._val_storage = []  # list of dicts: {\"images\": images, \"targets\": targets, \"preds\": preds}\n",
    "        self.metrics = InstanceSegMetrics()\n",
    "        # track last validation batch for plotting\n",
    "        self._last_val_batch = None\n",
    "\n",
    "    def forward(self, images: List[torch.Tensor], targets: Optional[List[Dict[str, torch.Tensor]]] = None):\n",
    "        return self.model(images, targets)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, targets = batch  # expects (list[Tensor], list[dict])\n",
    "        # MaskRCNN returns losses dict when targets provided\n",
    "        losses = self.model(images, targets)\n",
    "        # losses is a dict\n",
    "        loss = sum(losses.values())\n",
    "        # log each loss\n",
    "        for k, v in losses.items():\n",
    "            self.log(f\"train/{k}\", v, on_step=True, on_epoch=False, prog_bar=False, logger=True)\n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, targets = batch\n",
    "        # compute val losses by passing targets (gives dict)\n",
    "        with torch.no_grad():\n",
    "            val_losses = self.model(images, targets)\n",
    "            # aggregate scalar loss\n",
    "            val_loss = sum(val_losses.values()) if isinstance(val_losses, dict) else torch.tensor(0.0, device=self.device)\n",
    "\n",
    "            # run inference to get predictions (list[dict])\n",
    "            preds = self.model(images)\n",
    "\n",
    "        # Ensure preds is a list\n",
    "        if not isinstance(preds, list):\n",
    "            preds = [preds]\n",
    "\n",
    "        # Process predictions and update metrics per image\n",
    "        for img_tensor, gt, pred in zip(images, targets, preds):\n",
    "            proc_pred = self.pred_proc(pred)\n",
    "            # Ensure gt masks are binary float and shaped [M,1,H,W]\n",
    "            gt_masks = gt.get(\"masks\", torch.empty((0, 1, 1, 1)))\n",
    "            if gt_masks.dim() == 3:\n",
    "                gt_masks = gt_masks.unsqueeze(1)\n",
    "            # update metrics\n",
    "            self.metrics.update(proc_pred, {\"masks\": gt_masks})\n",
    "\n",
    "        # store last batch for visualization\n",
    "        self._last_val_batch = {\"images\": images, \"targets\": targets, \"preds\": preds, \"epoch\": self.current_epoch}\n",
    "\n",
    "        # Log val loss\n",
    "        self.log(\"val/loss\", val_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {\"val_loss\": val_loss}\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # Compute metrics and log them (CSVLogger will pick them up)\n",
    "        metrics = self.metrics.compute()\n",
    "        # log all metrics\n",
    "        for k, v in metrics.items():\n",
    "            self.log(f\"val/{k}\", v, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "\n",
    "        # Save visualization for last validation batch\n",
    "        if self._last_val_batch is not None:\n",
    "            self._save_last_val_batch_plot(self._last_val_batch)\n",
    "\n",
    "        # reset for next epoch\n",
    "        self.metrics.reset()\n",
    "        self._last_val_batch = None\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = [p for p in self.model.parameters() if p.requires_grad]\n",
    "        opt = torch.optim.AdamW(params, lr=self.lr, weight_decay=self.weight_decay)\n",
    "        # no LR scheduler by default (add if desired)\n",
    "        return opt\n",
    "\n",
    "    # ---------- helper to save visualization ----------\n",
    "    def _save_last_val_batch_plot(self, batch_info: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Expects batch_info to contain 'images' (list[Tensor]), 'targets' (list[dict]), 'preds' (list[dict]), 'epoch' (int)\n",
    "        Uses BV visualizer as described by the user:\n",
    "            bv = BV(figsize=(16,8))\n",
    "            fig = bv.visualize_batch((x,y))\n",
    "            plot_array = bv.fig_to_image(fig[0])\n",
    "        And saves plot_array to self.logger.log_dir/validation_plots/epoch_{epoch}.png\n",
    "        \"\"\"\n",
    "        images = batch_info[\"images\"]\n",
    "        targets = batch_info[\"targets\"]\n",
    "        preds = batch_info[\"preds\"]\n",
    "        epoch = int(batch_info.get(\"epoch\", self.current_epoch))\n",
    "\n",
    "        # Inverse transform images and predictions for visualization\n",
    "        inv_images = []\n",
    "        inv_targets = []\n",
    "        inv_preds = []\n",
    "\n",
    "        for img_tensor, gt, pred in zip(images, targets, preds):\n",
    "            # inv_transform should accept a tensor and return HxWxC numpy array or PIL image\n",
    "            try:\n",
    "                inv_img = self.inv_transform(img_tensor)\n",
    "            except Exception:\n",
    "                # fallback: convert tensor to numpy HWC\n",
    "                inv_img = self._tensor_to_vis(img_tensor)\n",
    "            inv_images.append(inv_img)\n",
    "\n",
    "            # For GTs: ensure masks are numpy HxW (or list of masks)\n",
    "            gt_masks = gt.get(\"masks\", torch.empty((0, 1, 1, 1)))\n",
    "            if gt_masks.dim() == 3:\n",
    "                gt_masks = gt_masks.unsqueeze(1)\n",
    "            # convert to CPU numpy list of HxW arrays\n",
    "            gt_masks_np = []\n",
    "            for m in gt_masks:\n",
    "                m_cpu = m.detach().cpu().numpy()\n",
    "                # if [1,H,W] -> squeeze\n",
    "                if m_cpu.ndim == 3:\n",
    "                    m_cpu = m_cpu[0]\n",
    "                gt_masks_np.append((m_cpu > 0.5).astype('uint8'))\n",
    "            inv_targets.append({\"masks\": gt_masks_np, \"boxes\": gt.get(\"boxes\", None)})\n",
    "\n",
    "            # For predicted masks, process and convert\n",
    "            proc_pred = self.pred_proc(pred)\n",
    "            p_masks = proc_pred.get(\"masks\", torch.empty((0, 1, 1, 1)))\n",
    "            if p_masks.dim() == 3:\n",
    "                p_masks = p_masks.unsqueeze(1)\n",
    "            p_masks_np = []\n",
    "            for m in p_masks:\n",
    "                m_cpu = m.detach().cpu().numpy()\n",
    "                if m_cpu.ndim == 3:\n",
    "                    m_cpu = m_cpu[0]\n",
    "                p_masks_np.append((m_cpu > 0.5).astype('uint8'))\n",
    "            inv_preds.append({\"masks\": p_masks_np, \"scores\": proc_pred.get(\"scores\", None), \"boxes\": proc_pred.get(\"boxes\", None)})\n",
    "\n",
    "        # Use BV visualizer. We expect BV to be present.\n",
    "        try:\n",
    "            bv = BV(figsize=(16, 8))\n",
    "            # The user's code expects visualize_batch((x,y)) where x,y are (images, targets).\n",
    "            fig_list = bv.visualize_batch((inv_images, inv_targets, inv_preds))\n",
    "            # if visualize_batch returns a list of figs; pick first\n",
    "            fig = fig_list[0] if isinstance(fig_list, (list, tuple)) else fig_list\n",
    "            plot_array = bv.fig_to_image(fig)\n",
    "            # fig_to_image might return PIL or ndarray; normalize to ndarray\n",
    "            if isinstance(plot_array, (list, tuple)):\n",
    "                plot_array = plot_array[0]\n",
    "            if isinstance(plot_array, Image.Image):\n",
    "                arr = np.array(plot_array)\n",
    "            else:\n",
    "                arr = np.asarray(plot_array)\n",
    "        except Exception as e:\n",
    "            # BV not available or failed: fallback to a simple grid of images with masks overlay\n",
    "            arr = self._fallback_grid_image(inv_images, inv_targets, inv_preds)\n",
    "\n",
    "        # Determine output folder using logger.log_dir\n",
    "        base_log_dir = getattr(self.logger, \"log_dir\", None) if self.logger is not None else None\n",
    "        if base_log_dir is None:\n",
    "            base_log_dir = os.path.abspath(\"./logs\")\n",
    "        out_dir = os.path.join(base_log_dir, \"validation_plots\")\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        out_path = os.path.join(out_dir, f\"epoch_{epoch:03d}.png\")\n",
    "\n",
    "        # Save arr as PNG\n",
    "        im = Image.fromarray(arr.astype(\"uint8\"))\n",
    "        im.save(out_path)\n",
    "        # Also log path into the logger for traceability\n",
    "        # Note: CSVLogger won't accept images directly but we log the path as a metric\n",
    "        # self.log(\"val/last_val_plot\", out_path, on_epoch=True, logger=True)\n",
    "\n",
    "    def _tensor_to_vis(self, t: torch.Tensor) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert tensor image CxHxW into HxWxC numpy (uint8). Basic normalization applied.\n",
    "        \"\"\"\n",
    "        if isinstance(t, torch.Tensor):\n",
    "            img = t.detach().cpu()\n",
    "            if img.dim() == 3:\n",
    "                c, h, w = img.shape\n",
    "                arr = img.numpy()\n",
    "                # assume in [0,1] or ImageNet normalization; try unnormalize if mean/std present in attribute\n",
    "                arr = arr.transpose(1, 2, 0)  # HWC\n",
    "                # scale\n",
    "                arr = np.clip(arr, 0, 1)\n",
    "                arr = (arr * 255).astype(np.uint8)\n",
    "            else:\n",
    "                arr = (img.numpy() * 255).astype(np.uint8)\n",
    "        else:\n",
    "            arr = np.asarray(t)\n",
    "        return arr\n",
    "\n",
    "    def _fallback_grid_image(self, inv_images, inv_targets, inv_preds):\n",
    "        \"\"\"\n",
    "        Create a simple fallback visualization grid if BV is not available.\n",
    "        Returns HxWxC ndarray uint8\n",
    "        \"\"\"\n",
    "        # Create small overlays: just show images in a vertical stack with bounding boxes drawn (PIL)\n",
    "        pil_imgs = []\n",
    "        for img_arr, gt, pred in zip(inv_images, inv_targets, inv_preds):\n",
    "            # convert to PIL\n",
    "            if isinstance(img_arr, np.ndarray):\n",
    "                pil = Image.fromarray(img_arr.astype(\"uint8\"))\n",
    "            elif isinstance(img_arr, Image.Image):\n",
    "                pil = img_arr\n",
    "            else:\n",
    "                pil = Image.fromarray(np.asarray(img_arr).astype(\"uint8\"))\n",
    "            draw = Image.new(\"RGBA\", pil.size)\n",
    "            # convert to RGBA and return\n",
    "            pil_imgs.append(pil.convert(\"RGB\"))\n",
    "\n",
    "        # stack vertically\n",
    "        widths = [im.width for im in pil_imgs]\n",
    "        heights = [im.height for im in pil_imgs]\n",
    "        maxw = max(widths)\n",
    "        tot_h = sum(heights)\n",
    "        out = Image.new(\"RGB\", (maxw, tot_h))\n",
    "        y = 0\n",
    "        for im in pil_imgs:\n",
    "            out.paste(im, (0, y))\n",
    "            y += im.height\n",
    "        return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eb8de4c-8dda-4a3f-83bf-f3f941a91327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.resnets import BottleneckFPNBackbone, TreeMaskRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22ca2b0d-0b53-47f6-8911-91ffedd7aaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brian\\anaconda3\\envs\\dl_env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Brian\\anaconda3\\envs\\dl_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = TreeMaskRCNN(pretrained=False)\n",
    "\n",
    "def inv_transform(tensor):\n",
    "    # tensor: C,H,W with ImageNet normalization\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406])[:,None,None]\n",
    "    std = torch.tensor([0.229, 0.224, 0.225])[:,None,None]\n",
    "    img = tensor.cpu() * std + mean\n",
    "    img = img.clamp(0,1).permute(1,2,0).numpy()\n",
    "    return (img * 255).astype('uint8')\n",
    "    \n",
    "lightning_module = InstanceSegLightningModule(model=model, inv_transform=inv_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b57371f3-13c9-4fb9-9825-fe4a6004d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "788a2618-b3fa-41f7-92e9-95b2074a7894",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_module.eval()\n",
    "with torch.no_grad():\n",
    "    yp = lightning_module(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fd2a3fa-8b67-4d5d-b4f9-1318cf459562",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    ypp = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "022215aa-0ddb-41ab-b513-fb68e0e479bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = PredictionProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b51fbfe-fa3c-43f9-aacd-7e5d9a7f4ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b728d553d0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(inv_transform(yp[0]['masks'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c05543f-f27d-4907-946a-5af4ecbff8f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
